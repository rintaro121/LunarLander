{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn\n",
    "import gym\n",
    "import numpy\n",
    "from pfrl.policies import SoftmaxCategoricalHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(-inf, inf, (8,), float32)\n",
      "action space: Discrete(4)\n",
      "initial observation: [ 0.00756063  1.411787    0.76580507  0.03850863 -0.00875421 -0.1734664\n",
      "  0.          0.        ]\n",
      "\n",
      "next observation [ 0.01502562  1.4133496   0.75566757  0.06936412 -0.01782299 -0.1813921\n",
      "  0.          0.        ]\n",
      "reward: -0.57627459966061\n",
      "done: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "print(\"observation space:\",env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"initial observation:\", obs)\n",
    "print()\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "\n",
    "print(\"next observation\",obs)\n",
    "print(\"reward:\",r)\n",
    "print(\"done:\",done)\n",
    "print(\"info:\",info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(torch.nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(obs_size,  50)\n",
    "        self.l2 = torch.nn.Linear(50,50)\n",
    "        self.l3 = torch.nn.Linear(50,n_actions)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        h = torch.nn.functional.relu(self.l1(h))\n",
    "        h = torch.nn.functional.relu(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        return pfrl.action_value.DiscreteActionValue(h)\n",
    "    \n",
    "obs_size = env.observation_space.low.size\n",
    "n_actions = env.action_space.n\n",
    "q_func = QFunction(obs_size, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lecun_init(layer, gain=1):\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        pfrl.initializers.init_lecun_normal(layer.weight, gain)\n",
    "        nn.init.zeros_(layer.bias)\n",
    "    else:\n",
    "        pfrl.initializers.init_lecun_normal(layer.weight_ih_l0, gain)\n",
    "        pfrl.initializers.init_lecun_normal(layer.weight_hh_l0, gain)\n",
    "        nn.init.zeros_(layer.bias_ih_l0)\n",
    "        nn.init.zeros_(layer.bias_hh_l0)\n",
    "    return layer\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    lecun_init(torch.nn.Linear(obs_size,32)),\n",
    "    torch.nn.ReLU(),\n",
    "    lecun_init(torch.nn.Linear(32, 64)),\n",
    "    torch.nn.ReLU(),\n",
    "    lecun_init(torch.nn.Linear(64, 128)),\n",
    "    torch.nn.ReLU(),\n",
    "    pfrl.nn.Branched(\n",
    "        torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, n_actions),\n",
    "            SoftmaxCategoricalHead(),\n",
    "        ),\n",
    "        torch.nn.Linear(128,1)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nagent = pfrl.agents.A2C(\\n    \\n    model = q_func,\\n    optimizer = optimizer,\\n    gamma = gamma,\\n    num_processes = 2,\\n)\\n\\nagent = pfrl.agents.REINFORCE(\\n    q_func,\\n    optimizer,\\n)\\n\\nagent = pfrl.agents.DoubleDQN(\\n    q_func,\\n    optimizer,\\n    replay_buffer,\\n    gamma,\\n    explorer,\\n    replay_start_size=500,\\n    update_interval=1,\\n    target_update_interval=100,\\n    phi=phi,\\n    gpu=gpu,\\n)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "explorer = pfrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.1, random_action_func=env.action_space.sample)\n",
    "\n",
    "\n",
    "replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=10**6)\n",
    "\n",
    "phi = lambda x : x.astype(numpy.float32, copy=False)\n",
    "\n",
    "gpu = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create an agent that will interact with the environment.\n",
    "\n",
    "agent = pfrl.agents.PPO(\n",
    "    model,\n",
    "    optimizer,\n",
    "    phi=phi,\n",
    "    gpu=gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10 R: -332.7852277708553\n",
      "episode: 20 R: -177.37747685149145\n",
      "episode: 30 R: -298.58002250933964\n",
      "episode: 40 R: -145.27106214679347\n",
      "episode: 50 R: -237.79607983681223\n",
      "statistics: [('average_value', -59.21073), ('average_entropy', 1.2624002), ('average_value_loss', 301.5330840301514), ('average_policy_loss', 0.019767593815922736), ('n_updates', 640), ('explained_variance', 0.6273739224000227)]\n",
      "episode: 60 R: -163.16121789487067\n",
      "episode: 70 R: -80.80046530120964\n",
      "episode: 80 R: -141.62816323449405\n",
      "episode: 90 R: -63.42941612096345\n",
      "episode: 100 R: -59.470657499999604\n",
      "statistics: [('average_value', -57.422634), ('average_entropy', 1.2427988), ('average_value_loss', 349.0746898651123), ('average_policy_loss', 0.003944343943148851), ('n_updates', 1280), ('explained_variance', 0.2431647020932174)]\n",
      "episode: 110 R: -79.12802471706831\n",
      "episode: 120 R: -136.50136112904698\n",
      "episode: 130 R: -145.7688019874663\n",
      "episode: 140 R: -89.34818581926976\n",
      "episode: 150 R: -237.95511503701795\n",
      "statistics: [('average_value', -60.63565), ('average_entropy', 1.1952393), ('average_value_loss', 209.0760319519043), ('average_policy_loss', 0.015447099758312106), ('n_updates', 2240), ('explained_variance', 0.07573517440877886)]\n",
      "episode: 160 R: -70.99550837411374\n",
      "episode: 170 R: -101.8809484340973\n",
      "episode: 180 R: -131.59782828891542\n",
      "episode: 190 R: -162.37022095789507\n",
      "episode: 200 R: -96.81075990681008\n",
      "statistics: [('average_value', -59.9086), ('average_entropy', 1.2177958), ('average_value_loss', 94.67469875335694), ('average_policy_loss', -0.01375258257612586), ('n_updates', 2880), ('explained_variance', 0.7699829366806391)]\n",
      "episode: 210 R: -47.89854695545704\n",
      "episode: 220 R: -132.25878918558035\n",
      "episode: 230 R: -70.64249630534005\n",
      "episode: 240 R: -140.00153305269546\n",
      "episode: 250 R: -37.73484051369208\n",
      "statistics: [('average_value', -68.71728), ('average_entropy', 1.0879402), ('average_value_loss', 52.04965194702149), ('average_policy_loss', 0.004727839753031731), ('n_updates', 3840), ('explained_variance', 0.8302363922943283)]\n",
      "episode: 260 R: -126.65543036691115\n",
      "episode: 270 R: -8.673695846056896\n",
      "episode: 280 R: 47.40641547026234\n",
      "episode: 290 R: 15.955010428821211\n",
      "episode: 300 R: -71.60811629049397\n",
      "statistics: [('average_value', -33.668404), ('average_entropy', 0.9961252), ('average_value_loss', 42.27159080505371), ('average_policy_loss', 0.0006520110182464123), ('n_updates', 5120), ('explained_variance', 0.8126105862602397)]\n",
      "episode: 310 R: 82.24125412220414\n",
      "episode: 320 R: 122.71334080356777\n",
      "episode: 330 R: 41.23860130722247\n",
      "episode: 340 R: 78.95198576381487\n",
      "episode: 350 R: 125.02755134852652\n",
      "statistics: [('average_value', -4.1246667), ('average_entropy', 0.956939), ('average_value_loss', 15.85516037940979), ('average_policy_loss', -0.0009826504159718753), ('n_updates', 6720), ('explained_variance', 0.9523962125847918)]\n",
      "episode: 360 R: 83.3528212632674\n",
      "episode: 370 R: 98.69410838453767\n",
      "episode: 380 R: 87.06337271872046\n",
      "episode: 390 R: 67.63472307759923\n",
      "episode: 400 R: 67.7404302119221\n",
      "statistics: [('average_value', 8.338386), ('average_entropy', 0.9319131), ('average_value_loss', 4.290220859050751), ('average_policy_loss', -0.004611672898754478), ('n_updates', 8320), ('explained_variance', 0.9781080645036075)]\n",
      "episode: 410 R: 90.70153765288944\n",
      "episode: 420 R: 83.68561492446813\n",
      "episode: 430 R: 91.00816141897486\n",
      "episode: 440 R: 103.7583935708687\n",
      "episode: 450 R: 103.95160224483624\n",
      "statistics: [('average_value', 14.66211), ('average_entropy', 0.88750637), ('average_value_loss', 2.038510205745697), ('average_policy_loss', -0.004002277608960867), ('n_updates', 9920), ('explained_variance', 0.9932574888630873)]\n",
      "episode: 460 R: 77.4264638941493\n",
      "episode: 470 R: 104.04505878966448\n",
      "episode: 480 R: 80.07675700378918\n",
      "episode: 490 R: 106.34543454775051\n",
      "episode: 500 R: 123.63138972373852\n",
      "statistics: [('average_value', 20.136755), ('average_entropy', 0.83610183), ('average_value_loss', 1.7117067688703538), ('average_policy_loss', -0.00664267310872674), ('n_updates', 11520), ('explained_variance', 0.9943635477132525)]\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 500\n",
    "max_episode_len = 200\n",
    "\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    R = 0\n",
    "    t = 0\n",
    "    while True:\n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        reset = t == max_episode_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        if done or reset:\n",
    "            break\n",
    "            \n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i, 'R:', R)\n",
    "    if i % 50 == 0:\n",
    "        print('statistics:', agent.get_statistics())\n",
    "print('Finished.') \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode: 0 R: 103.56545010431041\n",
      "evaluation episode: 1 R: 94.45278799210082\n",
      "evaluation episode: 2 R: 94.29366186646524\n",
      "evaluation episode: 3 R: 85.42420986292896\n",
      "evaluation episode: 4 R: 104.0510659315425\n",
      "evaluation episode: 5 R: 115.80786061155713\n",
      "evaluation episode: 6 R: 103.51273417589255\n",
      "evaluation episode: 7 R: 116.18353404377916\n",
      "evaluation episode: 8 R: 103.52254523323795\n",
      "evaluation episode: 9 R: 132.2453684225183\n"
     ]
    }
   ],
   "source": [
    "with agent.eval_mode():\n",
    "    for i in range(10):\n",
    "        obs = env.reset()\n",
    "        R = 0\n",
    "        t = 0\n",
    "        while True:\n",
    "            # Uncomment to watch the behavior in a GUI window\n",
    "            env.render()\n",
    "            action = agent.act(obs)\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            R += r\n",
    "            t += 1\n",
    "            reset = t == 200\n",
    "            agent.observe(obs, r, done, reset)\n",
    "            if done or reset:\n",
    "                break\n",
    "        print('evaluation episode:', i, 'R:', R)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an agent to the 'agent' directory\n",
    "agent.save('agent')\n",
    "\n",
    "# Uncomment to load an agent from the 'agent' directory\n",
    "# agent.load('agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:90 episode:0 R:-97.73226806886188\n",
      "statistics:[('average_value', -74.75675), ('average_entropy', 0.7915316), ('average_value_loss', 57.11389946937561), ('average_policy_loss', 0.00021705696359276773), ('n_updates', 3200), ('explained_variance', 0.6381624224785301)]\n",
      "outdir:result step:163 episode:1 R:-156.16694701401906\n",
      "statistics:[('average_value', -75.34754), ('average_entropy', 0.78401834), ('average_value_loss', 57.11389946937561), ('average_policy_loss', 0.00021705696359276773), ('n_updates', 3200), ('explained_variance', 0.6381624224785301)]\n",
      "outdir:result step:222 episode:2 R:-110.49030834650256\n",
      "statistics:[('average_value', -74.96526), ('average_entropy', 0.7631847), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:300 episode:3 R:-111.44865339331764\n",
      "statistics:[('average_value', -75.016045), ('average_entropy', 0.75832874), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:371 episode:4 R:-145.69950995487048\n",
      "statistics:[('average_value', -73.696465), ('average_entropy', 0.7400181), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:459 episode:5 R:-170.71261925788258\n",
      "statistics:[('average_value', -74.34251), ('average_entropy', 0.7158886), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:548 episode:6 R:-147.65604688203973\n",
      "statistics:[('average_value', -75.63564), ('average_entropy', 0.6944224), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:624 episode:7 R:-145.61752623462283\n",
      "statistics:[('average_value', -76.11017), ('average_entropy', 0.67482275), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:676 episode:8 R:-88.21206663832672\n",
      "statistics:[('average_value', -75.64851), ('average_entropy', 0.6682206), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:731 episode:9 R:-131.2274308819373\n",
      "statistics:[('average_value', -74.85945), ('average_entropy', 0.6726429), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:821 episode:10 R:-101.44375289947733\n",
      "statistics:[('average_value', -75.027824), ('average_entropy', 0.65222985), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:897 episode:11 R:-154.33313162251935\n",
      "statistics:[('average_value', -74.32164), ('average_entropy', 0.6283864), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:953 episode:12 R:-111.57576251608373\n",
      "statistics:[('average_value', -73.675644), ('average_entropy', 0.62074816), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1035 episode:13 R:-224.16827136583166\n",
      "statistics:[('average_value', -73.20655), ('average_entropy', 0.5989976), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "evaluation episode 0 length:72 R:-128.68218956695924\n",
      "evaluation episode 1 length:74 R:-124.2395080523915\n",
      "evaluation episode 2 length:75 R:-133.20448321845402\n",
      "evaluation episode 3 length:51 R:-96.55933324513666\n",
      "evaluation episode 4 length:60 R:-110.9295211768075\n",
      "evaluation episode 5 length:69 R:-152.7853719704963\n",
      "evaluation episode 6 length:62 R:-111.3730050066435\n",
      "evaluation episode 7 length:81 R:-144.74081939032237\n",
      "evaluation episode 8 length:74 R:-138.2451782148102\n",
      "evaluation episode 9 length:78 R:-116.3850989089572\n",
      "The best score is updated -3.4028235e+38 -> -125.71445087509785\n",
      "Saved the agent to result/best\n",
      "outdir:result step:1124 episode:14 R:-138.29133951556315\n",
      "statistics:[('average_value', -74.731834), ('average_entropy', 0.5755627), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1178 episode:15 R:-121.11306011687347\n",
      "statistics:[('average_value', -74.82914), ('average_entropy', 0.5689726), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1249 episode:16 R:-128.17935062951597\n",
      "statistics:[('average_value', -74.86695), ('average_entropy', 0.5702566), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1335 episode:17 R:-116.02916920666269\n",
      "statistics:[('average_value', -74.78125), ('average_entropy', 0.56862384), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1408 episode:18 R:-158.22347368859025\n",
      "statistics:[('average_value', -74.46768), ('average_entropy', 0.5701971), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1500 episode:19 R:-142.75496420933092\n",
      "statistics:[('average_value', -73.938156), ('average_entropy', 0.58369654), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1572 episode:20 R:-127.88871793669526\n",
      "statistics:[('average_value', -73.13825), ('average_entropy', 0.59512556), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1637 episode:21 R:-111.32174139891524\n",
      "statistics:[('average_value', -73.72817), ('average_entropy', 0.5987899), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1701 episode:22 R:-119.8377885972948\n",
      "statistics:[('average_value', -73.696625), ('average_entropy', 0.60270303), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1787 episode:23 R:-128.9269347834935\n",
      "statistics:[('average_value', -75.33158), ('average_entropy', 0.5900146), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1865 episode:24 R:-135.86909454133746\n",
      "statistics:[('average_value', -75.281624), ('average_entropy', 0.5904737), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:1955 episode:25 R:-114.61670029070672\n",
      "statistics:[('average_value', -75.699356), ('average_entropy', 0.59597003), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n",
      "outdir:result step:2000 episode:26 R:-5.0666481364908345\n",
      "statistics:[('average_value', -75.712906), ('average_entropy', 0.5988562), ('average_value_loss', 21.65699993133545), ('average_policy_loss', 0.004735526461154223), ('n_updates', 3520), ('explained_variance', 0.7693223469820605)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 0 length:76 R:-129.30842691962692\n",
      "evaluation episode 1 length:62 R:-109.25730375619855\n",
      "evaluation episode 2 length:63 R:-112.7268614174246\n",
      "evaluation episode 3 length:60 R:-120.11581729958104\n",
      "evaluation episode 4 length:58 R:-131.22260848475727\n",
      "evaluation episode 5 length:59 R:-134.34470931013547\n",
      "evaluation episode 6 length:76 R:-131.26019100748397\n",
      "evaluation episode 7 length:54 R:-121.33257227612492\n",
      "evaluation episode 8 length:59 R:-101.75015808368397\n",
      "evaluation episode 9 length:73 R:-167.2087619638767\n",
      "Saved the agent to result/2000_finish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<pfrl.agents.ppo.PPO at 0x7fbe738196d0>,\n",
       " [{'average_value': -73.20655,\n",
       "   'average_entropy': 0.5989976,\n",
       "   'average_value_loss': 21.65699993133545,\n",
       "   'average_policy_loss': 0.004735526461154223,\n",
       "   'n_updates': 3520,\n",
       "   'explained_variance': 0.7693223469820605,\n",
       "   'eval_score': -125.71445087509785},\n",
       "  {'average_value': -75.712906,\n",
       "   'average_entropy': 0.5988562,\n",
       "   'average_value_loss': 21.65699993133545,\n",
       "   'average_policy_loss': 0.004735526461154223,\n",
       "   'n_updates': 3520,\n",
       "   'explained_variance': 0.7693223469820605,\n",
       "   'eval_score': -125.85274105188934}])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Set up the logger to print info messages for understandability.\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
    "\n",
    "pfrl.experiments.train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=2000,           # Train the agent for 2000 steps\n",
    "    eval_n_steps=None,       # We evaluate for episodes, not time\n",
    "    eval_n_episodes=10,       # 10 episodes are sampled for each evaluation\n",
    "    train_max_episode_len=200,  # Maximum length of each episode\n",
    "    eval_interval=1000,   # Evaluate the agent after every 1000 steps\n",
    "    outdir='result',      # Save everything to 'result' directory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_lunerlander)",
   "language": "python",
   "name": "conda_lunerlander"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
